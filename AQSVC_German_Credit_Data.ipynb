{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Author: Pratham Gujar\n",
        "\n",
        "Email: pratham.gujar30@gmail.com"
      ],
      "metadata": {
        "id": "z5Kf2XynKiuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Installations"
      ],
      "metadata": {
        "id": "hD20zaWvDTsj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9FaD5rpDNGQ",
        "outputId": "4fbddda9-ba1b-464f-cd5c-9093534d9927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.6/231.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install qiskit -q\n",
        "%pip install pylatexenc -q\n",
        "%pip install qiskit_machine_learning -q\n",
        "%pip install imblearn -q\n",
        "%pip install qiskit-algorithms -q\n",
        "%pip install qiskit-aer -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
        "from qiskit.circuit.library import ZZFeatureMap\n",
        "from qiskit_aer import AerSimulator\n",
        "from qiskit_machine_learning.algorithms import QSVC\n",
        "from sklearn.metrics import classification_report, recall_score\n",
        "import itertools\n",
        "from time import time"
      ],
      "metadata": {
        "id": "TgRK1x6QDobj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Simple AQSVC For German Credit Data"
      ],
      "metadata": {
        "id": "S1h1VdM-Ds4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a processed version of the Statlog German Credit data where the features are better categorized for the QSVC model to use. The alphanumeric codes were removed and replaced with extra columns in the dataset.\n"
      ],
      "metadata": {
        "id": "NV9BGnDRD0fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjusted_features(df, save=True, output_file='German_Adjusted_Features.csv'):\n",
        "  \"\"\"\n",
        "  Adjusts the features based on correlation thresholds.\n",
        "\n",
        "  Args:\n",
        "      df (pandas.DataFrame): The input DataFrame.\n",
        "      save (bool, optional): Whether to save the adjusted features. Defaults to True.\n",
        "      output_file (str, optional): The output file path. Defaults to 'German_Adjusted_Features.csv'.\n",
        "\n",
        "  Returns:\n",
        "      pandas.DataFrame: The DataFrame with adjusted features.\n",
        "  \"\"\"\n",
        "\n",
        "  correlation_matrix = df.corr()\n",
        "\n",
        "  # Extract correlations with the target 'classification'\n",
        "  correlations_with_target = correlation_matrix['classification'].drop('classification').sort_values(ascending=False)\n",
        "\n",
        "  adjusted_features = correlations_with_target[\n",
        "      (correlations_with_target >= 0.55) | (correlations_with_target <= -0.05)\n",
        "  ].index.tolist()\n",
        "\n",
        "\n",
        "  adjusted_features_data = df[adjusted_features + ['classification']]\n",
        "\n",
        "  if save:\n",
        "    adjusted_features_data.to_csv(output_file, index=False)\n",
        "\n",
        "  return adjusted_features_data\n",
        "\n"
      ],
      "metadata": {
        "id": "Yf6OyWzqDzji"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I noticed that the QSVC model had trouble fitting the minority data properly since there were much more features with a positive correlation than a negative correlation. To better the fitting of the minority class, I reduced the features from the dataset which were positively correlated to give more focus to the minority features. I made sure to not trim too much so the score remained above at least 0.7."
      ],
      "metadata": {
        "id": "HxSaqKzOFT2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(X, y, test_size=0.2, random_state=42, n_dim=4):\n",
        "  \"\"\"\n",
        "  Preprocesses data using PCA based on the number of qubits (n_dim)\n",
        "\n",
        "  Args:\n",
        "      X (numpy.ndarray): Input features.\n",
        "      y (numpy.ndarray): Target labels.\n",
        "      test_size (float, optional): Proportion of the dataset to include in the test split. Defaults to 0.2.\n",
        "      random_state (int, optional): Seed for the random number generator. Defaults to 42.\n",
        "      n_dim (int, optional): Number of dimensions for the PCA. Defaults to 4.\n",
        "\n",
        "  Returns:\n",
        "      tuple: A tuple containing the preprocessed training data, test data, training labels, and test labels.\n",
        "  \"\"\"\n",
        "\n",
        "  sample_train, sample_test, label_train, label_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "\n",
        "  # Reduce dimensions\n",
        "  pca = PCA(n_components=n_dim).fit(sample_train)\n",
        "  sample_train = pca.transform(sample_train)\n",
        "  sample_test = pca.transform(sample_test)\n",
        "\n",
        "  # Normalize\n",
        "  std_scale = StandardScaler().fit(sample_train)\n",
        "  sample_train = std_scale.transform(sample_train)\n",
        "  sample_test = std_scale.transform(sample_test)\n",
        "\n",
        "  # Scale\n",
        "  samples = np.append(sample_train, sample_test, axis=0)\n",
        "  minmax_scaler = MinMaxScaler((-1,1)).fit(samples)\n",
        "  sample_train = minmax_scaler.transform(sample_train)\n",
        "  sample_test = minmax_scaler.transform(sample_test)\n",
        "\n",
        "  # Select\n",
        "  train_size = 100\n",
        "  sample_train = sample_train[:train_size]\n",
        "  label_train = label_train[:train_size].values\n",
        "\n",
        "  test_size = 20\n",
        "  sample_test = sample_test[:test_size]\n",
        "  label_test = label_test[:test_size].values\n",
        "\n",
        "  return sample_train, sample_test, label_train, label_test"
      ],
      "metadata": {
        "id": "4yTgCECFFNT4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_qsvc(sample_train, label_train, sample_test, n_dim=4, save=True, output_model='german_qsvc.joblib', entanglement='linear', reps=2):\n",
        "  \"\"\"\n",
        "  Runs QSVC on the provided data.\n",
        "\n",
        "  Args:\n",
        "      sample_train (numpy.ndarray): Training data.\n",
        "      label_train (numpy.ndarray): Training labels.\n",
        "      sample_test (numpy.ndarray): Test data.\n",
        "      n_dim (int, optional): Number of dimensions for the QSVC. Defaults to 4.\n",
        "      save (bool, optional): Whether to save the trained QSVC model. Defaults to True.\n",
        "\n",
        "  Returns:\n",
        "      numpy.ndarray: Predictions for the test data.\n",
        "  \"\"\"\n",
        "\n",
        "  zz_map = ZZFeatureMap(feature_dimension=n_dim, reps=reps, entanglement=entanglement, insert_barriers=True)\n",
        "\n",
        "  zz_kernel = FidelityQuantumKernel(feature_map=zz_map)\n",
        "\n",
        "  qsvc = QSVC(quantum_kernel=zz_kernel)\n",
        "  qsvc.fit(sample_train, label_train)\n",
        "\n",
        "  if save:\n",
        "    qsvc.save(output_model)\n",
        "\n",
        "  predictions = qsvc.predict(sample_test)\n",
        "\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "BlMrBFSGFRDd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_qsvc(X, y, test_params=None, n=10, seed=42, verbose=True):\n",
        "    \"\"\"\n",
        "    Optimization function to find the best repetitions (reps), entanglement, and number of qubits (n_dim)\n",
        "    for the QSVC model.\n",
        "\n",
        "    Args:\n",
        "        X (numpy.ndarray): Input features.\n",
        "        y (numpy.ndarray): Target labels.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the best minority recall, the best parameters\n",
        "\n",
        "    \"\"\"\n",
        "    best_recall = 0\n",
        "    best_params = {}\n",
        "    best_report = \"\"\n",
        "    if test_params is None:\n",
        "      reps_range = [1, 2, 3]\n",
        "      entanglement_options = ['linear', 'full', 'circular', 'sca']\n",
        "      n_dim_range = [2, 4, 6]\n",
        "    else:\n",
        "      reps_range = test_params['reps']\n",
        "      entanglement_options = test_params['entanglement']\n",
        "      n_dim_range = test_params['n_dim']\n",
        "\n",
        "    params_combinations = list(itertools.product(reps_range, entanglement_options, n_dim_range))[:n]\n",
        "    np.random.shuffle(params_combinations)\n",
        "\n",
        "    for i, (reps, entanglement, n_dim) in enumerate(params_combinations):\n",
        "        start = time()\n",
        "\n",
        "        sample_train, sample_test, label_train, label_test = preprocess(X, y, n_dim=n_dim, random_state=seed)\n",
        "\n",
        "        predictions = run_qsvc(sample_train, label_train, sample_test, n_dim=n_dim, reps=reps, entanglement=entanglement, save=False)\n",
        "\n",
        "        minority_recall = recall_score(label_test, predictions, pos_label=-1)\n",
        "\n",
        "        params = {\n",
        "            'reps': reps,\n",
        "            'entanglement': entanglement,\n",
        "            'n_dim': n_dim\n",
        "        }\n",
        "\n",
        "        if minority_recall > best_recall:\n",
        "            best_recall = minority_recall\n",
        "            best_params = params\n",
        "            # best_report = classification_report(label_test, predictions)\n",
        "        if verbose:\n",
        "          print(f\"Iteration {i + 1}: Current Recall: {minority_recall:.2f}, Params: {params}, Time Took: {time()-start:.2f}s\")\n",
        "\n",
        "    return best_recall, best_params"
      ],
      "metadata": {
        "id": "t7BJC6PoFZmp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  df = pd.read_csv('German_Preprocessed.csv')\n",
        "  df = adjusted_features(df, save=False)\n",
        "\n",
        "  X = df.drop('classification', axis=1)\n",
        "  y = df['classification']\n",
        "\n",
        "  best_recall, best_params = optimize_qsvc(X, y, n=36, seed=42)\n",
        "\n",
        "  print(\"\\nFinal Results:\")\n",
        "  print(\"Best Minority Recall:\", best_recall)\n",
        "  print(\"Best Parameters:\", best_params)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpCGajFMFcCc",
        "outputId": "fa5ac5e4-03f3-46d5-8d76-a748e82ec469"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Current Recall: 0.00, Params: {'reps': 1, 'entanglement': 'sca', 'n_dim': 6}, Time Took: 58.63s\n",
            "Iteration 2: Current Recall: 0.17, Params: {'reps': 3, 'entanglement': 'linear', 'n_dim': 2}, Time Took: 29.37s\n",
            "Iteration 3: Current Recall: 0.00, Params: {'reps': 2, 'entanglement': 'circular', 'n_dim': 6}, Time Took: 77.14s\n",
            "Iteration 4: Current Recall: 0.17, Params: {'reps': 3, 'entanglement': 'full', 'n_dim': 2}, Time Took: 29.56s\n",
            "Iteration 5: Current Recall: 0.00, Params: {'reps': 2, 'entanglement': 'sca', 'n_dim': 6}, Time Took: 74.83s\n",
            "Iteration 6: Current Recall: 0.00, Params: {'reps': 1, 'entanglement': 'full', 'n_dim': 6}, Time Took: 72.27s\n",
            "Iteration 7: Current Recall: 0.33, Params: {'reps': 1, 'entanglement': 'full', 'n_dim': 2}, Time Took: 15.15s\n",
            "Iteration 8: Current Recall: 0.17, Params: {'reps': 2, 'entanglement': 'circular', 'n_dim': 4}, Time Took: 50.19s\n",
            "Iteration 9: Current Recall: 0.00, Params: {'reps': 3, 'entanglement': 'linear', 'n_dim': 4}, Time Took: 59.89s\n",
            "Iteration 10: Current Recall: 0.67, Params: {'reps': 2, 'entanglement': 'linear', 'n_dim': 4}, Time Took: 44.80s\n",
            "Iteration 11: Current Recall: 0.17, Params: {'reps': 3, 'entanglement': 'circular', 'n_dim': 2}, Time Took: 29.73s\n",
            "Iteration 12: Current Recall: 0.00, Params: {'reps': 3, 'entanglement': 'linear', 'n_dim': 6}, Time Took: 96.22s\n",
            "Iteration 13: Current Recall: 0.33, Params: {'reps': 1, 'entanglement': 'linear', 'n_dim': 4}, Time Took: 26.84s\n",
            "Iteration 14: Current Recall: 0.17, Params: {'reps': 1, 'entanglement': 'full', 'n_dim': 4}, Time Took: 36.51s\n",
            "Iteration 15: Current Recall: 0.00, Params: {'reps': 1, 'entanglement': 'circular', 'n_dim': 6}, Time Took: 43.85s\n",
            "Iteration 16: Current Recall: 0.33, Params: {'reps': 2, 'entanglement': 'full', 'n_dim': 2}, Time Took: 23.00s\n",
            "Iteration 17: Current Recall: 0.00, Params: {'reps': 3, 'entanglement': 'full', 'n_dim': 6}, Time Took: 185.53s\n",
            "Iteration 18: Current Recall: 0.00, Params: {'reps': 3, 'entanglement': 'sca', 'n_dim': 4}, Time Took: 70.39s\n",
            "Iteration 19: Current Recall: 0.00, Params: {'reps': 2, 'entanglement': 'linear', 'n_dim': 6}, Time Took: 68.63s\n",
            "Iteration 20: Current Recall: 0.33, Params: {'reps': 1, 'entanglement': 'linear', 'n_dim': 2}, Time Took: 15.22s\n",
            "Iteration 21: Current Recall: 0.17, Params: {'reps': 3, 'entanglement': 'sca', 'n_dim': 2}, Time Took: 29.83s\n",
            "Iteration 22: Current Recall: 0.33, Params: {'reps': 1, 'entanglement': 'sca', 'n_dim': 2}, Time Took: 15.32s\n",
            "Iteration 23: Current Recall: 0.00, Params: {'reps': 2, 'entanglement': 'full', 'n_dim': 6}, Time Took: 127.96s\n",
            "Iteration 24: Current Recall: 0.50, Params: {'reps': 1, 'entanglement': 'circular', 'n_dim': 4}, Time Took: 30.15s\n",
            "Iteration 25: Current Recall: 0.00, Params: {'reps': 3, 'entanglement': 'sca', 'n_dim': 6}, Time Took: 104.81s\n",
            "Iteration 26: Current Recall: 0.17, Params: {'reps': 2, 'entanglement': 'sca', 'n_dim': 4}, Time Took: 50.37s\n",
            "Iteration 27: Current Recall: 0.33, Params: {'reps': 2, 'entanglement': 'linear', 'n_dim': 2}, Time Took: 23.12s\n",
            "Iteration 28: Current Recall: 0.33, Params: {'reps': 2, 'entanglement': 'sca', 'n_dim': 2}, Time Took: 21.99s\n",
            "Iteration 29: Current Recall: 0.00, Params: {'reps': 3, 'entanglement': 'full', 'n_dim': 4}, Time Took: 87.21s\n",
            "Iteration 30: Current Recall: 0.00, Params: {'reps': 3, 'entanglement': 'circular', 'n_dim': 4}, Time Took: 69.35s\n",
            "Iteration 31: Current Recall: 0.33, Params: {'reps': 1, 'entanglement': 'circular', 'n_dim': 2}, Time Took: 14.98s\n",
            "Iteration 32: Current Recall: 0.17, Params: {'reps': 1, 'entanglement': 'linear', 'n_dim': 6}, Time Took: 43.74s\n",
            "Iteration 33: Current Recall: 0.17, Params: {'reps': 2, 'entanglement': 'full', 'n_dim': 4}, Time Took: 61.21s\n",
            "Iteration 34: Current Recall: 0.50, Params: {'reps': 1, 'entanglement': 'sca', 'n_dim': 4}, Time Took: 30.04s\n",
            "Iteration 35: Current Recall: 0.00, Params: {'reps': 3, 'entanglement': 'circular', 'n_dim': 6}, Time Took: 105.72s\n",
            "Iteration 36: Current Recall: 0.33, Params: {'reps': 2, 'entanglement': 'circular', 'n_dim': 2}, Time Took: 22.33s\n",
            "\n",
            "Final Results:\n",
            "Best Minority Recall: 0.6666666666666666\n",
            "Best Parameters: {'reps': 2, 'entanglement': 'linear', 'n_dim': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Ways to improve"
      ],
      "metadata": {
        "id": "VnJYAiA_Fihg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The current code utilizes the **Aer Simulator** rather than an actual quantum backend. This limits the number of qubits we can use to create a feature map for the data. There could be more accurate models that utilize 20 or 30 qubits which would be too long to classically simulate.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kLs1du2hFn5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another proposition would be to deeply investigate the dataset and identify possible pairwise interactions amongst features. This would allow us to alter the **ZZFeatureMap** to better map the controlled gates with the particular features. This should make evaluating the kernel matrix more efficient. Furthermore, this could make the model's outcome more accurate but this would depend on several more factors. By doing this, we are reducing the complexity of the circuit which would cause some encodings to no longer happen. Thus, we lose some information which may lead to a more innacurate model. We must also be careful of overfitting to ensure the model can properly predict outcomes of unseen data."
      ],
      "metadata": {
        "id": "MatCffblKYUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we do run this on a quantum computer, we will need to consider noise and the fault tolerance of the computer. We will need to use error correction syndromes (such as the Perfect Code algorithm) to identify potential errors and fix them before improperly computing the kernel matrix."
      ],
      "metadata": {
        "id": "Kdq_bo4FJ-tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report and Results"
      ],
      "metadata": {
        "id": "SEcITps7lIbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract"
      ],
      "metadata": {
        "id": "WC5dFEcNnVPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the code above, I found that using 2 repetitions with linear entanglement and 4 qubits yielded the best result of 0.67 minority recall. Different results could be achieved by changing the test/train split but it will not vary by too much."
      ],
      "metadata": {
        "id": "_OwKlbEblLvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools Used"
      ],
      "metadata": {
        "id": "96mwf6nEneny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this, I utilized Qiskit for modeling my machine learning implementation. I also used a preprocessed version of the Statlot German Credit data where the alphanumerical values in some columns are restructured into their own indvidual columns with numerical values. For data handling, I used pandas, numpy, and sklearn. Much of the preprocessing utilizes functions from sklearn."
      ],
      "metadata": {
        "id": "XTOf5SNGnPn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Strategy"
      ],
      "metadata": {
        "id": "XGy18Ru7o7kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I filter the German Credit Data to find features more closely correlated to the minority target value. I tweaked this to increase minority recall and maximize the overall accuracy score for the SVC fitting. This is found in the `adjusted_features()` function."
      ],
      "metadata": {
        "id": "01UWoGEMpPmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I preprocess the restructured German Credit Data using PCA to reduce the dimensionality to match the number of qubits. For example, if the data is reduced to 4 dimensions, then there are 4 qubits to represent the features.\n",
        "\n",
        "The features are then normalized using a Z-score normalization to have a mean of 0 and a standard deviation of 1. The features are then scaled to -1 and 1 to fascilitate machine learning fitting. Finally, a sample of the training and testing data is selected to minimized computational computing."
      ],
      "metadata": {
        "id": "6xBMyaeqo87v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a `ZZFeatureMap` to map the features to the quantum map. This handles pairwise interactions between qubits which considers interactions between features and how they affect the produced target. I create a quantum kernel using the feature map and the kernel is evaluated when using the `qsvc` function. The evaluated kernel matrix is used in the generated quantum circuit to find the best model to predict test data. Refer to the `run_qsvc()` function to see the implementation."
      ],
      "metadata": {
        "id": "oIJbYjr8riHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `optimize_qsvc()` function wraps around the `run_qsvc()` function to try various quantum hyperparameters such as repetitions `reps`, entanglement `entanglement`, and number of qubits `n_dims`. There are default values assigned if the user does not feed in `test_params`."
      ],
      "metadata": {
        "id": "FDEjfGhptDiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "xKl23FUkwG2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With my strategy, I determined that 2 repetitions of the feature circuit, with a linear entanglement, and 4 qubits yielded the best minority recall of 0.67. The testing was not comprehensive since I was limited by computational power and there could be a better configuration. However, I noticed that as the qubit number deviated from 4, the minority score decreased until reaching 0. Thus, it appears that 4 qubits may already be an optimized hyperparameter for the QSVC solution."
      ],
      "metadata": {
        "id": "1ZgcKDEOwIcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "tV-nKDpSueof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above, found a configuration where the minority recall is above 0.5 without sacrificing much from the accuracy score (0.75). The `ZZFeatureMap` provided best results as it considered interactions between features and mapped the features the best. A custom Pauli Map could be implemented to better map the features but more investigation is required. The results obtained did satisfy the condition for achieving a minority score of 0.5."
      ],
      "metadata": {
        "id": "UzRvIlZxugNK"
      }
    }
  ]
}