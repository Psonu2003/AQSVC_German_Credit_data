{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGOhSt5b957cIgxN5URemO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Psonu2003/AQSVC_German_Credit_data/blob/main/AQSVC_German_Credit_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Author: Pratham Gujar\n",
        "\n",
        "Email: pratham.gujar30@gmail.com"
      ],
      "metadata": {
        "id": "z5Kf2XynKiuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Installations"
      ],
      "metadata": {
        "id": "hD20zaWvDTsj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9FaD5rpDNGQ",
        "outputId": "95bcde3a-bd99-45ee-fcc5-b1225fc7e4c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.6/231.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install qiskit -q\n",
        "%pip install pylatexenc -q\n",
        "%pip install qiskit_machine_learning -q\n",
        "%pip install imblearn -q\n",
        "%pip install qiskit-algorithms -q\n",
        "%pip install qiskit-aer -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
        "from qiskit.circuit.library import ZZFeatureMap\n",
        "from qiskit_aer import AerSimulator\n",
        "from qiskit_machine_learning.algorithms import QSVC\n",
        "from sklearn.metrics import classification_report, recall_score\n",
        "import itertools\n",
        "from time import time"
      ],
      "metadata": {
        "id": "TgRK1x6QDobj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Simple AQSVC For German Credit Data"
      ],
      "metadata": {
        "id": "S1h1VdM-Ds4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a processed version of the Statlog German Credit data where the features are better categorized for the QSVC model to use. The alphanumeric codes were removed and replaced with extra columns in the dataset.\n"
      ],
      "metadata": {
        "id": "NV9BGnDRD0fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjusted_features(df, save=True, output_file='German_Adjusted_Features.csv'):\n",
        "  \"\"\"\n",
        "  Adjusts the features based on correlation thresholds.\n",
        "\n",
        "  Args:\n",
        "      df (pandas.DataFrame): The input DataFrame.\n",
        "      save (bool, optional): Whether to save the adjusted features. Defaults to True.\n",
        "      output_file (str, optional): The output file path. Defaults to 'German_Adjusted_Features.csv'.\n",
        "\n",
        "  Returns:\n",
        "      pandas.DataFrame: The DataFrame with adjusted features.\n",
        "  \"\"\"\n",
        "\n",
        "  correlation_matrix = df.corr()\n",
        "\n",
        "  # Extract correlations with the target 'classification'\n",
        "  correlations_with_target = correlation_matrix['classification'].drop('classification').sort_values(ascending=False)\n",
        "\n",
        "  adjusted_features = correlations_with_target[\n",
        "      (correlations_with_target >= 0.55) | (correlations_with_target <= -0.05)\n",
        "  ].index.tolist()\n",
        "\n",
        "\n",
        "  adjusted_features_data = df[adjusted_features + ['classification']]\n",
        "\n",
        "  if save:\n",
        "    adjusted_features_data.to_csv(output_file, index=False)\n",
        "\n",
        "  return adjusted_features_data\n",
        "\n"
      ],
      "metadata": {
        "id": "Yf6OyWzqDzji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I noticed that the QSVC model had trouble fitting the minority data properly since there were much more features with a positive correlation than a negative correlation. To better the fitting of the minority class, I reduced the features from the dataset which were positively correlated to give more focus to the minority features. I made sure to not trim too much so the score remained above at least 0.7."
      ],
      "metadata": {
        "id": "HxSaqKzOFT2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(X, y, test_size=0.2, random_state=42, n_dim=4):\n",
        "  \"\"\"\n",
        "  Preprocesses data using PCA based on the number of qubits (n_dim)\n",
        "\n",
        "  Args:\n",
        "      X (numpy.ndarray): Input features.\n",
        "      y (numpy.ndarray): Target labels.\n",
        "      test_size (float, optional): Proportion of the dataset to include in the test split. Defaults to 0.2.\n",
        "      random_state (int, optional): Seed for the random number generator. Defaults to 42.\n",
        "      n_dim (int, optional): Number of dimensions for the PCA. Defaults to 4.\n",
        "\n",
        "  Returns:\n",
        "      tuple: A tuple containing the preprocessed training data, test data, training labels, and test labels.\n",
        "  \"\"\"\n",
        "\n",
        "  sample_train, sample_test, label_train, label_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "\n",
        "  # Reduce dimensions\n",
        "  pca = PCA(n_components=n_dim).fit(sample_train)\n",
        "  sample_train = pca.transform(sample_train)\n",
        "  sample_test = pca.transform(sample_test)\n",
        "\n",
        "  # Normalize\n",
        "  std_scale = StandardScaler().fit(sample_train)\n",
        "  sample_train = std_scale.transform(sample_train)\n",
        "  sample_test = std_scale.transform(sample_test)\n",
        "\n",
        "  # Scale\n",
        "  samples = np.append(sample_train, sample_test, axis=0)\n",
        "  minmax_scaler = MinMaxScaler((-1,1)).fit(samples)\n",
        "  sample_train = minmax_scaler.transform(sample_train)\n",
        "  sample_test = minmax_scaler.transform(sample_test)\n",
        "\n",
        "  # Select\n",
        "  train_size = 100\n",
        "  sample_train = sample_train[:train_size]\n",
        "  label_train = label_train[:train_size].values\n",
        "\n",
        "  test_size = 20\n",
        "  sample_test = sample_test[:test_size]\n",
        "  label_test = label_test[:test_size].values\n",
        "\n",
        "  return sample_train, sample_test, label_train, label_test"
      ],
      "metadata": {
        "id": "4yTgCECFFNT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_qsvc(sample_train, label_train, sample_test, n_dim=4, save=True, output_model='german_qsvc.joblib', entanglement='linear', reps=2):\n",
        "  \"\"\"\n",
        "  Runs QSVC on the provided data.\n",
        "\n",
        "  Args:\n",
        "      sample_train (numpy.ndarray): Training data.\n",
        "      label_train (numpy.ndarray): Training labels.\n",
        "      sample_test (numpy.ndarray): Test data.\n",
        "      n_dim (int, optional): Number of dimensions for the QSVC. Defaults to 4.\n",
        "      save (bool, optional): Whether to save the trained QSVC model. Defaults to True.\n",
        "\n",
        "  Returns:\n",
        "      numpy.ndarray: Predictions for the test data.\n",
        "  \"\"\"\n",
        "\n",
        "  zz_map = ZZFeatureMap(feature_dimension=n_dim, reps=reps, entanglement=entanglement, insert_barriers=True)\n",
        "\n",
        "  zz_kernel = FidelityQuantumKernel(feature_map=zz_map)\n",
        "\n",
        "  qsvc = QSVC(quantum_kernel=zz_kernel)\n",
        "  qsvc.fit(sample_train, label_train)\n",
        "\n",
        "  if save:\n",
        "    qsvc.save(output_model)\n",
        "\n",
        "  predictions = qsvc.predict(sample_test)\n",
        "\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "BlMrBFSGFRDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_qsvc(X, y, test_params=None, n=10):\n",
        "    \"\"\"\n",
        "    Optimization function to find the best repetitions (reps), entanglement, and number of qubits (n_dim)\n",
        "    for the QSVC model.\n",
        "\n",
        "    Args:\n",
        "        X (numpy.ndarray): Input features.\n",
        "        y (numpy.ndarray): Target labels.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the best minority recall, the best parameters\n",
        "\n",
        "    \"\"\"\n",
        "    best_recall = 0\n",
        "    best_params = {}\n",
        "    best_report = \"\"\n",
        "    if test_params is None:\n",
        "      reps_range = [1, 2, 3]\n",
        "      entanglement_options = ['linear', 'full', 'circular', 'sca']\n",
        "      n_dim_range = [2, 4, 6]\n",
        "    else:\n",
        "      reps_range = test_params['reps']\n",
        "      entanglement_options = test_params['entanglement']\n",
        "      n_dim_range = test_params['n_dim']\n",
        "\n",
        "    params_combinations = list(itertools.product(reps_range, entanglement_options, n_dim_range))[:n]\n",
        "    np.random.shuffle(params_combinations)\n",
        "\n",
        "    for i, (reps, entanglement, n_dim) in enumerate(params_combinations):\n",
        "        start = time()\n",
        "        # Preprocess data\n",
        "        sample_train, sample_test, label_train, label_test = preprocess(X, y, n_dim=n_dim)\n",
        "\n",
        "        # Run QSVC\n",
        "        predictions = run_qsvc(sample_train, label_train, sample_test, n_dim=n_dim, reps=reps, entanglement=entanglement, save=False)\n",
        "\n",
        "        # Compute minority recall\n",
        "        minority_recall = recall_score(label_test, predictions, pos_label=-1)  # Adjust pos_label to match your data\n",
        "\n",
        "        params = {\n",
        "            'reps': reps,\n",
        "            'entanglement': entanglement,\n",
        "            'n_dim': n_dim\n",
        "        }\n",
        "\n",
        "        if minority_recall > best_recall:\n",
        "            best_recall = minority_recall\n",
        "            best_params = params\n",
        "            # best_report = classification_report(label_test, predictions)\n",
        "\n",
        "        print(f\"Iteration {i + 1}: Current Recall: {minority_recall:.2f}, Params: {params}, Time Took: {time()-start:.2f}s\")\n",
        "\n",
        "    return best_recall, best_params"
      ],
      "metadata": {
        "id": "t7BJC6PoFZmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  df = pd.read_csv('German_Preprocessed.csv')\n",
        "  df = adjusted_features(df, save=False)\n",
        "\n",
        "  X = df.drop('classification', axis=1)\n",
        "  y = df['classification']\n",
        "\n",
        "  best_recall, best_params = optimize_qsvc(X, y, n=10)\n",
        "\n",
        "  print(\"\\nFinal Results:\")\n",
        "  print(\"Best Minority Recall:\", best_recall)\n",
        "  print(\"Best Parameters:\", best_params)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "wpCGajFMFcCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Ways to improve"
      ],
      "metadata": {
        "id": "VnJYAiA_Fihg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The current code utilizes the **Aer Simulator** rather than an actual quantum backend. This limits the number of qubits we can use to create a feature map for the data. There could be more accurate models that utilize 20 or 30 qubits which would be too long to classically simulate.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kLs1du2hFn5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another proposition would be to deeply investigate the dataset and identify possible pairwise interactions amongst features. This would allow us to alter the **ZZFeatureMap** to better map the controlled gates with the particular features. This should make evaluating the kernel matrix more efficient. Furthermore, this could make the model's outcome more accurate but this would depend on several more factors. By doing this, we are reducing the complexity of the circuit which would cause some encodings to no longer happen. Thus, we lose some information which may lead to a more innacurate model. We must also be careful of overfitting to ensure the model can properly predict outcomes of unseen data."
      ],
      "metadata": {
        "id": "MatCffblKYUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we do run this on a quantum computer, we will need to consider noise and the fault tolerance of the computer. We will need to use error correction syndromes (such as the Perfect Code algorithm) to identify potential errors and fix them before improperly computing the kernel matrix."
      ],
      "metadata": {
        "id": "Kdq_bo4FJ-tm"
      }
    }
  ]
}